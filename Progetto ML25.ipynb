{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "36xvd24zrqqh",
        "outputId": "d411b7ad-2faa-4f08-861b-0182bae2e98b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore) (4.14.1)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=dd6082979faabeefbb677f3b4037d94030a347f918d7e8ea5bfeee6901f03d45\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=15569b1c0c90d7e1ca579837cd75c840e34287418674a069accb9654e9623d0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "# 1: RUN AT EVERY NEW RUNTIME\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "# Install fvcore\n",
        "!pip install -U fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRCAYuL9uH_u",
        "outputId": "75e55cf7-dc1e-4aa2-c3eb-eb2df5c09df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 2: NEED TO RUN AT AVERY NEW RUNTIME\n",
        "# Setup google drive connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from google.colab import auth\n",
        "\n",
        "# Autenticazione\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq91B_2IAZcN"
      },
      "outputs": [],
      "source": [
        "# TO BE RUNNED JUST THE FIRST TIME!\n",
        "# Setup git integration\n",
        "#%cd /content/drive/MyDrive\n",
        "#!git clone https://github.com/s348174/MLDL2024_project1.git\n",
        "#%cd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvTUimb9uK5Z",
        "outputId": "985cd220-c827-4f04-fdca-242411841f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MLDL2024_project1\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), 289 bytes | 0 bytes/s, done.\n",
            "From https://github.com/s348174/MLDL2024_project1\n",
            "   f70e2e5..1d2e272  master     -> origin/master\n",
            "Updating f70e2e5..1d2e272\n",
            "Fast-forward\n",
            " train.py | 1 \u001b[32m+\u001b[m\n",
            " 1 file changed, 1 insertion(+)\n",
            "/root\n"
          ]
        }
      ],
      "source": [
        "# 3. RUN EVERYTIME IN ORDER TO AVOID CONFLICTS\n",
        "# Update drive git directory\n",
        "%cd /content/drive/MyDrive/MLDL2024_project1\n",
        "!git pull\n",
        "%cd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. RUN TO REMOUNT DRIVE AND APPLY CHANGES\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PS3wj1SMPI6",
        "outputId": "61e24d28-e8c9-4598-8c1e-f736a2a96102"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ce9lSdNSyOin",
        "outputId": "12203c17-509d-4fa1-b761-dd17a89e7b99"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4ab4916bf96b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m#print(\"Contenuto dello ZIP:\", z.namelist())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#image_folder_path=tmpdir+\"/Cityscapes/Cityspaces/images/train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#train_data = datasets.ImageFolder(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0;31m# No need to compute the CRC if we don't have a reference value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         \u001b[0;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expected_crc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 5A. RUN THIS CELL TO TRAIN DEEPLAB ON CITYSCAPES\n",
        "import sys\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "# Add the directory containing train.py to the system path\n",
        "google_drive_path = '/content/drive/MyDrive/MLDL2024_project1'\n",
        "sys.path.append(google_drive_path)\n",
        "datasets_folder_path = '/content/drive/MyDrive/MLDL2024_project1/datasets'\n",
        "sys.path.append(datasets_folder_path)\n",
        "models_path = '/content/drive/MyDrive/MLDL2024_project1/models/deeplabv2'\n",
        "sys.path.append(models_path)\n",
        "from train import deeplab_train, deeplab_test\n",
        "\n",
        "# Open zipfile\n",
        "import zipfile\n",
        "zip_path = '/content/drive/MyDrive/machine_learning_shared_2025/Cityscapes.zip'\n",
        "workspace_path = google_drive_path\n",
        "with zipfile.ZipFile(zip_path) as z:\n",
        "  # Mostra i file contenuti\n",
        "  #print(\"Contenuto dello ZIP:\", z.namelist())\n",
        "  with TemporaryDirectory() as tmpdir:\n",
        "    z.extractall(tmpdir)\n",
        "    #image_folder_path=tmpdir+\"/Cityscapes/Cityspaces/images/train\"\n",
        "    #train_data = datasets.ImageFolder(\n",
        "    #    root=image_folder_path,\n",
        "    #    transform=ToTensor()\n",
        "    #)\n",
        "    #test_folder_path=tmpdir+\"/Cityscapes/Cityspaces/images/test\"\n",
        "    #test_data = datasets.ImageFolder(\n",
        "    #    root=image_folder_path,\n",
        "    #    transform=ToTensor()\n",
        "    #)\n",
        "\n",
        "    # Creates symlink to data folder\n",
        "    real_path = \"tmpdir\"\n",
        "    #destinazione_symlink = \"/tmp/tmpcp08lfus\"\n",
        "    #if not os.path.exists(destinazione_symlink):\n",
        "    #    os.symlink(real_path, destinazione_symlink)\n",
        "\n",
        "    num_epochs = 50\n",
        "    batch_size = 2\n",
        "    dataset_path = tmpdir+\"/Cityscapes/Cityspaces\"\n",
        "    pretrained_path = workspace_path + \"/deeplab_resnet_pretrained_imagenet.pth\"\n",
        "    deeplab_train(dataset_path, workspace_path, pretrained_path, checkpoint=False, balanced=False, num_epochs=num_epochs, batch_size=batch_size)\n",
        "    model_path = workspace_path + \"/export/deeplabv2_final.pth\"\n",
        "    deeplab_test(dataset_path, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5B. RUN THIS CELL TO TRAIN BISENET ON CITYSCAPES\n",
        "import sys\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "# Add the directory containing train.py to the system path\n",
        "google_drive_path = '/content/drive/MyDrive/MLDL2024_project1'\n",
        "sys.path.append(google_drive_path)\n",
        "datasets_folder_path = '/content/drive/MyDrive/MLDL2024_project1/datasets'\n",
        "sys.path.append(datasets_folder_path)\n",
        "models_path = '/content/drive/MyDrive/MLDL2024_project1/models/bisenet'\n",
        "sys.path.append(models_path)\n",
        "from train import bisenet_train, bisenet_test\n",
        "\n",
        "# Open zipfile\n",
        "import zipfile\n",
        "zip_path = '/content/drive/MyDrive/machine_learning_shared_2025/Cityscapes.zip'\n",
        "workspace_path = google_drive_path\n",
        "with zipfile.ZipFile(zip_path) as z:\n",
        "  # Mostra i file contenuti\n",
        "  #print(\"Contenuto dello ZIP:\", z.namelist())\n",
        "  with TemporaryDirectory() as tmpdir:\n",
        "    z.extractall(tmpdir)\n",
        "    #image_folder_path=tmpdir+\"/Cityscapes/Cityspaces/images/train\"\n",
        "    #train_data = datasets.ImageFolder(\n",
        "    #    root=image_folder_path,\n",
        "    #    transform=ToTensor()\n",
        "    #)\n",
        "    #test_folder_path=tmpdir+\"/Cityscapes/Cityspaces/images/test\"\n",
        "    #test_data = datasets.ImageFolder(\n",
        "    #    root=image_folder_path,\n",
        "    #    transform=ToTensor()\n",
        "    #)\n",
        "\n",
        "    # Creates symlink to data folder\n",
        "    real_path = \"tmpdir\"\n",
        "    #destinazione_symlink = \"/tmp/tmpcp08lfus\"\n",
        "    #if not os.path.exists(destinazione_symlink):\n",
        "    #    os.symlink(real_path, destinazione_symlink)\n",
        "\n",
        "    num_epochs = 50\n",
        "    batch_size = 2\n",
        "    dataset_path = tmpdir+\"/Cityscapes/Cityspaces\"\n",
        "    pretrained_path = workspace_path + \"/deeplab_resnet_pretrained_imagenet.pth\"\n",
        "    bisenet_train(dataset_path, workspace_path, pretrained_path, checkpoint=False, balanced=False, num_epochs=num_epochs, batch_size=batch_size, augmentation='0000')\n",
        "    model_path = workspace_path + \"/export/bisenet_final.pth\"\n",
        "    bisenet_test(dataset_path, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf6DnI4eL6EG",
        "outputId": "181ad3f3-7505-4699-f181-fbf86cbd3e94",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1572 images and 1572 labels\n",
            "Image map: 1572\n",
            "Matched 1572 image-label pairs\n",
            "Training with 12 workers and batch size 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 164MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:00<00:00, 212MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiSeNet pretrain loading...\n",
            "Starting training from scratch with pretrained weights\n",
            "Moving model to device...\n",
            "Current iteration: 0, Max iterations: 39300\n",
            "BiSeNet model saved at epoch 0\n",
            "BiSeNet model saved at epoch 5\n",
            "BiSeNet model saved at epoch 10\n",
            "BiSeNet model saved at epoch 15\n",
            "BiSeNet model saved at epoch 20\n",
            "BiSeNet model saved at epoch 25\n",
            "BiSeNet model saved at epoch 30\n",
            "BiSeNet model saved at epoch 35\n",
            "BiSeNet model saved at epoch 40\n",
            "BiSeNet model saved at epoch 45\n",
            "BiSeNet model saved as bisenet_final.pth\n",
            "Found 500 images and 500 labels\n",
            "Image map: 500\n",
            "Matched 500 image-label pairs\n",
            "Running BiSeNet inference...\n",
            "Iteration 0/500, Latency: 0.1883s, FPS: 5.31\n",
            "Iteration 10/500, Latency: 0.0051s, FPS: 194.78\n",
            "Iteration 20/500, Latency: 0.0051s, FPS: 194.83\n",
            "Iteration 30/500, Latency: 0.0053s, FPS: 187.30\n",
            "Iteration 40/500, Latency: 0.0054s, FPS: 186.55\n",
            "Iteration 50/500, Latency: 0.0054s, FPS: 185.88\n",
            "Iteration 60/500, Latency: 0.0053s, FPS: 188.84\n",
            "Iteration 70/500, Latency: 0.0054s, FPS: 186.14\n",
            "Iteration 80/500, Latency: 0.0054s, FPS: 186.55\n",
            "Iteration 90/500, Latency: 0.0051s, FPS: 195.18\n",
            "Iteration 100/500, Latency: 0.0051s, FPS: 195.37\n",
            "Iteration 110/500, Latency: 0.0051s, FPS: 195.47\n",
            "Iteration 120/500, Latency: 0.0051s, FPS: 195.66\n",
            "Iteration 130/500, Latency: 0.0051s, FPS: 195.87\n",
            "Iteration 140/500, Latency: 0.0052s, FPS: 190.69\n",
            "Iteration 150/500, Latency: 0.0052s, FPS: 191.71\n",
            "Iteration 160/500, Latency: 0.0052s, FPS: 193.99\n",
            "Iteration 170/500, Latency: 0.0051s, FPS: 194.58\n",
            "Iteration 180/500, Latency: 0.0051s, FPS: 195.03\n",
            "Iteration 190/500, Latency: 0.0053s, FPS: 190.11\n",
            "Iteration 200/500, Latency: 0.0051s, FPS: 194.59\n",
            "Iteration 210/500, Latency: 0.0050s, FPS: 200.02\n",
            "Iteration 220/500, Latency: 0.0053s, FPS: 189.49\n",
            "Iteration 230/500, Latency: 0.0052s, FPS: 194.08\n",
            "Iteration 240/500, Latency: 0.0052s, FPS: 193.76\n",
            "Iteration 250/500, Latency: 0.0051s, FPS: 197.60\n",
            "Iteration 260/500, Latency: 0.0051s, FPS: 196.51\n",
            "Iteration 270/500, Latency: 0.0052s, FPS: 193.99\n",
            "Iteration 280/500, Latency: 0.0051s, FPS: 195.30\n",
            "Iteration 290/500, Latency: 0.0051s, FPS: 196.27\n",
            "Iteration 300/500, Latency: 0.0051s, FPS: 196.08\n",
            "Iteration 310/500, Latency: 0.0051s, FPS: 195.93\n",
            "Iteration 320/500, Latency: 0.0051s, FPS: 196.57\n",
            "Iteration 330/500, Latency: 0.0051s, FPS: 196.47\n",
            "Iteration 340/500, Latency: 0.0052s, FPS: 193.30\n",
            "Iteration 350/500, Latency: 0.0052s, FPS: 194.05\n",
            "Iteration 360/500, Latency: 0.0051s, FPS: 196.12\n",
            "Iteration 370/500, Latency: 0.0052s, FPS: 193.74\n",
            "Iteration 380/500, Latency: 0.0052s, FPS: 192.71\n",
            "Iteration 390/500, Latency: 0.0051s, FPS: 195.26\n",
            "Iteration 400/500, Latency: 0.0051s, FPS: 196.86\n",
            "Iteration 410/500, Latency: 0.0051s, FPS: 195.57\n",
            "Iteration 420/500, Latency: 0.0051s, FPS: 195.73\n",
            "Iteration 430/500, Latency: 0.0051s, FPS: 195.42\n",
            "Iteration 440/500, Latency: 0.0054s, FPS: 185.67\n",
            "Iteration 450/500, Latency: 0.0064s, FPS: 157.24\n",
            "Iteration 460/500, Latency: 0.0057s, FPS: 176.00\n",
            "Iteration 470/500, Latency: 0.0051s, FPS: 196.47\n",
            "Iteration 480/500, Latency: 0.0051s, FPS: 196.38\n",
            "Iteration 490/500, Latency: 0.0051s, FPS: 195.89\n",
            "\n",
            "Evaluation complete on BiseNet with 50 epochs, 2 batch size, and balanced=False.\n",
            "\n",
            "Pixel Accuracy: 93.39%\n",
            "Per-class IoU: [0.9688159  0.76877358 0.87805801 0.4432002  0.42318709 0.39154699\n",
            " 0.41867748 0.55749818 0.89099286 0.55687879 0.91829212 0.67700141\n",
            " 0.42040277 0.89189826 0.41526042 0.51353515 0.37071804 0.41226316\n",
            " 0.62034055]\n",
            "Mean IoU: 0.6072284715421808\n",
            "Mean Latency: 5.59 ms\n",
            "Latency Std Dev: 8.18 ms\n",
            "Mean FPS: 191.49\n",
            "FPS Std Dev: 11.97\n",
            "| module                                      | #parameters or shape   | #flops     |\n",
            "|:--------------------------------------------|:-----------------------|:-----------|\n",
            "| model                                       | 12.582M                | 25.78G     |\n",
            "|  saptial_path                               |  0.371M                |  5.088G    |\n",
            "|   saptial_path.convblock1                   |   1.856K               |   0.243G   |\n",
            "|    saptial_path.convblock1.conv1            |    1.728K              |    0.226G  |\n",
            "|    saptial_path.convblock1.bn               |    0.128K              |    16.777M |\n",
            "|   saptial_path.convblock2                   |   73.984K              |   2.424G   |\n",
            "|    saptial_path.convblock2.conv1            |    73.728K             |    2.416G  |\n",
            "|    saptial_path.convblock2.bn               |    0.256K              |    8.389M  |\n",
            "|   saptial_path.convblock3                   |   0.295M               |   2.42G    |\n",
            "|    saptial_path.convblock3.conv1            |    0.295M              |    2.416G  |\n",
            "|    saptial_path.convblock3.bn               |    0.512K              |    4.194M  |\n",
            "|  context_path.features                      |  11.69M                |  19.002G   |\n",
            "|   context_path.features.conv1               |   9.408K               |   1.233G   |\n",
            "|    context_path.features.conv1.weight       |    (64, 3, 7, 7)       |            |\n",
            "|   context_path.features.bn1                 |   0.128K               |   16.777M  |\n",
            "|    context_path.features.bn1.weight         |    (64,)               |            |\n",
            "|    context_path.features.bn1.bias           |    (64,)               |            |\n",
            "|   context_path.features.layer1              |   0.148M               |   4.849G   |\n",
            "|    context_path.features.layer1.0           |    73.984K             |    2.424G  |\n",
            "|    context_path.features.layer1.1           |    73.984K             |    2.424G  |\n",
            "|   context_path.features.layer2              |   0.526M               |   4.305G   |\n",
            "|    context_path.features.layer2.0           |    0.23M               |    1.885G  |\n",
            "|    context_path.features.layer2.1           |    0.295M              |    2.42G   |\n",
            "|   context_path.features.layer3              |   2.1M                 |   4.3G     |\n",
            "|    context_path.features.layer3.0           |    0.919M              |    1.882G  |\n",
            "|    context_path.features.layer3.1           |    1.181M              |    2.418G  |\n",
            "|   context_path.features.layer4              |   8.394M               |   4.298G   |\n",
            "|    context_path.features.layer4.0           |    3.673M              |    1.881G  |\n",
            "|    context_path.features.layer4.1           |    4.721M              |    2.417G  |\n",
            "|   context_path.features.fc                  |   0.513M               |            |\n",
            "|    context_path.features.fc.weight          |    (1000, 512)         |            |\n",
            "|    context_path.features.fc.bias            |    (1000,)             |            |\n",
            "|  attention_refinement_module1               |  66.304K               |  0.59M     |\n",
            "|   attention_refinement_module1.conv         |   65.792K              |   65.536K  |\n",
            "|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)    |            |\n",
            "|    attention_refinement_module1.conv.bias   |    (256,)              |            |\n",
            "|   attention_refinement_module1.bn           |   0.512K               |   0.512K   |\n",
            "|    attention_refinement_module1.bn.weight   |    (256,)              |            |\n",
            "|    attention_refinement_module1.bn.bias     |    (256,)              |            |\n",
            "|   attention_refinement_module1.avgpool      |                        |   0.524M   |\n",
            "|  attention_refinement_module2               |  0.264M                |  0.525M    |\n",
            "|   attention_refinement_module2.conv         |   0.263M               |   0.262M   |\n",
            "|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)    |            |\n",
            "|    attention_refinement_module2.conv.bias   |    (512,)              |            |\n",
            "|   attention_refinement_module2.bn           |   1.024K               |   1.024K   |\n",
            "|    attention_refinement_module2.bn.weight   |    (512,)              |            |\n",
            "|    attention_refinement_module2.bn.bias     |    (512,)              |            |\n",
            "|   attention_refinement_module2.avgpool      |                        |   0.262M   |\n",
            "|  supervision1                               |  4.883K                |            |\n",
            "|   supervision1.weight                       |   (19, 256, 1, 1)      |            |\n",
            "|   supervision1.bias                         |   (19,)                |            |\n",
            "|  supervision2                               |  9.747K                |            |\n",
            "|   supervision2.weight                       |   (19, 512, 1, 1)      |            |\n",
            "|   supervision2.bias                         |   (19,)                |            |\n",
            "|  feature_fusion_module                      |  0.176M                |  1.435G    |\n",
            "|   feature_fusion_module.convblock           |   0.175M               |   1.435G   |\n",
            "|    feature_fusion_module.convblock.conv1    |    0.175M              |    1.434G  |\n",
            "|    feature_fusion_module.convblock.bn       |    38                  |    0.311M  |\n",
            "|   feature_fusion_module.conv1               |   0.38K                |   0.361K   |\n",
            "|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)      |            |\n",
            "|    feature_fusion_module.conv1.bias         |    (19,)               |            |\n",
            "|   feature_fusion_module.conv2               |   0.38K                |   0.361K   |\n",
            "|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)      |            |\n",
            "|    feature_fusion_module.conv2.bias         |    (19,)               |            |\n",
            "|   feature_fusion_module.avgpool             |                        |   0.156M   |\n",
            "|  conv                                       |  0.38K                 |  0.189G    |\n",
            "|   conv.weight                               |   (19, 19, 1, 1)       |            |\n",
            "|   conv.bias                                 |   (19,)                |            |\n",
            "Total FLOPs: 25.78 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. RUN THIS CELL TO TRAIN BISENET WITH ADVERSARIAL DOMAIN ADAPTATION\n",
        "import sys\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "import zipfile\n",
        "\n",
        "# Add the directory containing train.py to the system path\n",
        "google_drive_path = '/content/drive/MyDrive/MLDL2024_project1'\n",
        "sys.path.append(google_drive_path)\n",
        "datasets_folder_path = '/content/drive/MyDrive/MLDL2024_project1/datasets'\n",
        "sys.path.append(datasets_folder_path)\n",
        "models_path = '/content/drive/MyDrive/MLDL2024_project1/models/bisenet'\n",
        "sys.path.append(models_path)\n",
        "from train import bisenet_adversarial_adaptation\n",
        "\n",
        "# Open zipfiles\n",
        "gta_zip_path = '/content/drive/MyDrive/machine_learning_shared_2025/GTA5.zip'\n",
        "cityscapes_zip_path = '/content/drive/MyDrive/machine_learning_shared_2025/Cityscapes.zip'\n",
        "workspace_path = google_drive_path\n",
        "with TemporaryDirectory() as tmpdir1:\n",
        "    with zipfile.ZipFile(gta_zip_path) as z:\n",
        "        z.extractall(tmpdir1)\n",
        "    with TemporaryDirectory() as tmpdir2:\n",
        "        with zipfile.ZipFile(cityscapes_zip_path) as z:\n",
        "            z.extractall(tmpdir2)\n",
        "\n",
        "            # Creates symlink to data folder\n",
        "            real_path = \"tmpdir\"\n",
        "            #destinazione_symlink = \"/tmp/tmpcp08lfus\"\n",
        "            #if not os.path.exists(destinazione_symlink):\n",
        "            #    os.symlink(real_path, destinazione_symlink)\n",
        "\n",
        "            num_epochs = 50\n",
        "            batch_size = 2\n",
        "            dataset_path = tmpdir1+\"/GTA5\"\n",
        "            target_path = tmpdir2+\"/Cityscapes/Cityspaces\"\n",
        "            pretrained_path = workspace_path + \"/deeplab_resnet_pretrained_imagenet.pth\"\n",
        "            #pretrained_path = '/content/drive/MyDrive/machine_learning_shared_2025/' + 'bisenet_gta_epoch_30.pth'\n",
        "            bisenet_adversarial_adaptation(dataset_path, target_path, workspace_path, pretrained_path, checkpoint=False, balanced=False, num_epochs=num_epochs, batch_size=batch_size, augmentation='11111')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55lz64EcRbo9",
        "outputId": "8e95ecda-691a-4757-87a2-78f0ea3e1cef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Found 2500 images and 2500 labels\n",
            "Matched 2500 image-label pairs\n",
            "Found 1572 images and 1572 labels\n",
            "Image map: 1572\n",
            "Matched 1572 image-label pairs\n",
            "Training with 12 workers and batch size 2.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 103MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171M/171M [00:01<00:00, 140MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiSeNet pretrain loading...\n",
            "Starting training from scratch with pretrained weights\n",
            "Moving model to device...\n",
            "Current iteration: 0, Max iterations: 62500\n",
            "Starting adversarial training with 12 workers...\n",
            "One of the datasets has been exhausted, breaking epoch 0 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 0\n",
            "One of the datasets has been exhausted, breaking epoch 1 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 2 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 2\n",
            "One of the datasets has been exhausted, breaking epoch 3 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 4 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 4\n",
            "One of the datasets has been exhausted, breaking epoch 5 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 6 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 6\n",
            "One of the datasets has been exhausted, breaking epoch 7 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 8 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 8\n",
            "One of the datasets has been exhausted, breaking epoch 9 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 10 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 10\n",
            "One of the datasets has been exhausted, breaking epoch 11 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 12 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 12\n",
            "One of the datasets has been exhausted, breaking epoch 13 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 14 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 14\n",
            "One of the datasets has been exhausted, breaking epoch 15 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 16 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 16\n",
            "One of the datasets has been exhausted, breaking epoch 17 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 18 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 18\n",
            "One of the datasets has been exhausted, breaking epoch 19 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 20 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 20\n",
            "One of the datasets has been exhausted, breaking epoch 21 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 22 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 22\n",
            "One of the datasets has been exhausted, breaking epoch 23 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 24 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 24\n",
            "One of the datasets has been exhausted, breaking epoch 25 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 26 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 26\n",
            "One of the datasets has been exhausted, breaking epoch 27 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 28 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 28\n",
            "One of the datasets has been exhausted, breaking epoch 29 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 30 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 30\n",
            "One of the datasets has been exhausted, breaking epoch 31 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 32 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 32\n",
            "One of the datasets has been exhausted, breaking epoch 33 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 34 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 34\n",
            "One of the datasets has been exhausted, breaking epoch 35 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 36 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 36\n",
            "One of the datasets has been exhausted, breaking epoch 37 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 38 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 38\n",
            "One of the datasets has been exhausted, breaking epoch 39 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 40 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 40\n",
            "One of the datasets has been exhausted, breaking epoch 41 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 42 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 42\n",
            "One of the datasets has been exhausted, breaking epoch 43 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 44 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 44\n",
            "One of the datasets has been exhausted, breaking epoch 45 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 46 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 46\n",
            "One of the datasets has been exhausted, breaking epoch 47 at iteration 1250.\n",
            "One of the datasets has been exhausted, breaking epoch 48 at iteration 1250.\n",
            "BiSeNet model on GTA saved at epoch 48\n",
            "One of the datasets has been exhausted, breaking epoch 49 at iteration 1250.\n",
            "BiSeNet model saved as bisenet_adversarial_final_11111.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qAVqCvd3F3x8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6B. RUN THIS CELL TO TRAIN BISENET ON GTA5\n",
        "import sys\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "# Add the directory containing train.py to the system path\n",
        "google_drive_path = '/content/drive/MyDrive/MLDL2024_project1'\n",
        "sys.path.append(google_drive_path)\n",
        "datasets_folder_path = '/content/drive/MyDrive/MLDL2024_project1/datasets'\n",
        "sys.path.append(datasets_folder_path)\n",
        "models_path = '/content/drive/MyDrive/MLDL2024_project1/models/bisenet'\n",
        "sys.path.append(models_path)\n",
        "from train import bisenet_on_gta\n",
        "\n",
        "# Open zipfile\n",
        "import zipfile\n",
        "zip_path = '/content/drive/MyDrive/machine_learning_shared_2025/GTA5.zip'\n",
        "workspace_path = google_drive_path\n",
        "with zipfile.ZipFile(zip_path) as z:\n",
        "  # Mostra i file contenuti\n",
        "  #print(\"Contenuto dello ZIP:\", z.namelist())\n",
        "  with TemporaryDirectory() as tmpdir:\n",
        "    z.extractall(tmpdir)\n",
        "    #image_folder_path=tmpdir+\"/Cityscapes/Cityspaces/images/train\"\n",
        "    #train_data = datasets.ImageFolder(\n",
        "    #    root=image_folder_path,\n",
        "    #    transform=ToTensor()\n",
        "    #)\n",
        "    #test_folder_path=tmpdir+\"/Cityscapes/Cityspaces/images/test\"\n",
        "    #test_data = datasets.ImageFolder(\n",
        "    #    root=image_folder_path,\n",
        "    #    transform=ToTensor()\n",
        "    #)\n",
        "\n",
        "    # Creates symlink to data folder\n",
        "    real_path = \"tmpdir\"\n",
        "    #destinazione_symlink = \"/tmp/tmpcp08lfus\"\n",
        "    #if not os.path.exists(destinazione_symlink):\n",
        "    #    os.symlink(real_path, destinazione_symlink)\n",
        "\n",
        "    num_epochs = 50\n",
        "    batch_size = 2\n",
        "    dataset_path = tmpdir+\"/GTA5\"\n",
        "    pretrained_path = workspace_path + \"/deeplab_resnet_pretrained_imagenet.pth\"\n",
        "    #pretrained_path = '/content/drive/MyDrive/machine_learning_shared_2025/' + 'bisenet_gta_epoch_30.pth'\n",
        "    bisenet_on_gta(dataset_path, workspace_path, pretrained_path, checkpoint=False, balanced=False, num_epochs=num_epochs, batch_size=batch_size, augmentation='0110')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lto6dL9p-tOi",
        "outputId": "c23dde57-8c8a-4d52-be85-b6bbdca8a7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2135854357.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m#print(\"Contenuto dello ZIP:\", z.namelist())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#image_folder_path=tmpdir+\"/Cityscapes/Cityspaces/images/train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#train_data = datasets.ImageFolder(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mfdst_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pt4D4j89jTY",
        "outputId": "3cb3d88a-6fe0-4f78-fd91-89313657601e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPU cores: 12\n"
          ]
        }
      ],
      "source": [
        "#to check core availability\n",
        "import multiprocessing\n",
        "max_num_workers = multiprocessing.cpu_count() #colab pro has 4 (the default has just 2) (for Emanuele)\n",
        "print(f\"Number of CPU cores: {max_num_workers}\") #when Emanuele runs the code, it prints 8"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}